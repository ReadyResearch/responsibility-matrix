<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Figtree:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <title>AI Risk Responsibility Matrix</title>
    <style>
        body {
            font-family: 'Figtree', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
        }
        
        .matrix-container {
            background: white;
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            max-width: 1200px;
            margin: 0 auto;
        }
        
        .header {
            background: #a31e38;
            color: white;
            padding: 15px 20px;
        }
        
        .header h1 {
            margin: 0;
            font-size: 24px;
            font-weight: bold;
        }
        
        .header p {
            margin: 5px 0 0 0;
            font-size: 14px;
            opacity: 0.9;
        }
        
        .controls {
            background: #4a4a4a;
            padding: 10px 20px;
            display: flex;
            align-items: center;
            gap: 20px;
        }
        
        .controls select {
            background: #666;
            color: white;
            border: none;
            padding: 8px 12px;
            border-radius: 4px;
        }
        
        .actor-dimmed {
            opacity: 0.3;
            transition: opacity 0.3s;
        }
        
        .actor-highlighted {
            opacity: 1;
            box-shadow: 0 0 0 2px #a31e38;
            transition: opacity 0.3s, box-shadow 0.3s;
        }
        
        .tooltip {
            position: relative;
            cursor: pointer;
        }
        
        .tooltip-content {
            visibility: hidden;
            opacity: 0;
            position: absolute;
            top: -10px;
            left: 50%;
            transform: translateX(-50%) translateY(-100%);
            background-color: #333;
            color: white;
            padding: 15px;
            border-radius: 6px;
            font-size: 11px;
            white-space: nowrap;
            z-index: 1000;
            box-shadow: 0 4px 8px rgba(0,0,0,0.3);
            transition: opacity 0.3s, visibility 0.3s;
            min-width: 200px;
        }
        
        .tooltip-content::after {
            content: "";
            position: absolute;
            top: 100%;
            left: 50%;
            margin-left: -5px;
            border-width: 5px;
            border-style: solid;
            border-color: #333 transparent transparent transparent;
        }
        
        .tooltip:hover .tooltip-content {
            visibility: visible;
            opacity: 1;
        }
        
        .tooltip-header {
            font-weight: bold;
            margin-bottom: 8px;
            border-bottom: 1px solid #555;
            padding-bottom: 4px;
        }
        
        .tooltip-stats {
            display: flex;
            flex-direction: column;
            gap: 3px;
        }
        
        .tooltip-stat {
            display: flex;
            justify-content: space-between;
            gap: 15px;
        }
        
        .tooltip-stat.primary {
            font-weight: bold;
            color: #ffd700;
        }
        
        .tooltip-footer {
            margin-top: 8px;
            padding-top: 4px;
            border-top: 1px solid #555;
            font-size: 10px;
            color: #ccc;
        }
        
        .legend {
            background: #f8f8f8;
            padding: 15px 20px;
            border-bottom: 1px solid #ddd;
        }
        
        .legend-title {
            font-weight: bold;
            margin-bottom: 10px;
            font-size: 14px;
        }
        
        .legend-section {
            margin-bottom: 15px;
        }
        
        .legend-section:last-child {
            margin-bottom: 0;
        }
        
        .legend-items {
            display: flex;
            gap: 15px;
            flex-wrap: wrap;
        }
        
        .legend-item {
            display: flex;
            align-items: center;
            gap: 5px;
            font-size: 12px;
        }
        
        .legend-item input[type="checkbox"] {
            margin: 0;
            cursor: pointer;
        }
        
        .legend-item label {
            display: flex;
            align-items: center;
            gap: 5px;
            cursor: pointer;
            user-select: none;
        }
        
        .legend-color {
            width: 16px;
            height: 16px;
            border-radius: 2px;
        }
        
        .evidence-legend {
            display: flex;
            gap: 20px;
            flex-wrap: wrap;
            align-items: center;
        }
        
        .evidence-item {
            display: flex;
            align-items: center;
            gap: 5px;
            font-size: 12px;
        }
        
        .evidence-icon {
            width: 16px;
            height: 16px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 10px;
            font-weight: bold;
            color: white;
        }
        
        .evidence-consensus {
            background-color: #27ae60;
        }
        
        .evidence-no-consensus {
            background-color: #3498db;
        }
        
        .evidence-insufficient {
            background-color: #95a5a6;
        }
        
        .responsibility-table {
            width: 100%;
            border-collapse: collapse;
            font-size: 11px;
        }
        
        .responsibility-table th,
        .responsibility-table td {
            border: 1px solid #ddd;
            padding: 12px 8px;
            text-align: center;
            vertical-align: middle;
        }
        
        .actor-header {
            background: #4a4a4a;
            color: white;
            font-weight: bold;
            font-size: 10px;
            text-align: center;
            width: 25%;
        }
        
        .risk-domain-header {
            background: #4a4a4a;
            color: white;
            font-weight: bold;
            text-align: left;
            padding-left: 15px;
            padding-top: 12px;
            padding-bottom: 12px;
            cursor: pointer;
            transition: background-color 0.2s;
            font-size: 11px;
            width: 25%;
        }
        
        .risk-domain-header:hover {
            background: #3a3a3a;
        }
        
        .risk-domain-header.collapsed::after {
            content: ' ▶';
            float: right;
            padding-right: 10px;
        }
        
        .risk-domain-header.expanded::after {
            content: ' ▼';
            float: right;
            padding-right: 10px;
        }
        
        .risk-item {
            background: #d3d3d3;
            color: #333;
            text-align: left;
            padding-left: 25px;
            font-size: 10px;
            display: table-row;
            transition: opacity 0.3s;
        }
        
        .risk-item.hidden {
            display: none;
        }
        
        .risk-item td:first-child {
            background: #d3d3d3;
            color: #333;
            position: relative;
            cursor: help;
        }
        
        .risk-definition-tooltip {
            position: relative;
            cursor: help;
        }
        
        .risk-definition-tooltip .definition-content {
            visibility: hidden;
            opacity: 0;
            position: absolute;
            top: -10px;
            left: 50%;
            transform: translateX(-50%) translateY(-100%);
            background-color: #2c3e50;
            color: white;
            padding: 12px;
            border-radius: 6px;
            font-size: 11px;
            white-space: normal;
            z-index: 1001;
            box-shadow: 0 4px 8px rgba(0,0,0,0.3);
            transition: opacity 0.3s, visibility 0.3s;
            max-width: 300px;
            width: 300px;
            text-align: left;
            line-height: 1.4;
        }
        
        .risk-definition-tooltip:hover .definition-content {
            visibility: visible;
            opacity: 1;
        }
        
        .risk-definition-tooltip.align-right .definition-content {
            left: auto;
            right: 0;
            transform: translateY(-100%);
        }
        
        .risk-definition-tooltip.align-right .definition-content::after {
            left: auto;
            right: 20px;
            margin-left: 0;
            margin-right: -5px;
        }
        
        .risk-definition-tooltip.align-left .definition-content {
            left: 0;
            right: auto;
            transform: translateY(-100%);
        }
        
        .risk-definition-tooltip.align-left .definition-content::after {
            left: 20px;
            right: auto;
            margin-left: -5px;
            margin-right: 0;
        }
        
        .definition-header {
            font-weight: bold;
            margin-bottom: 6px;
            color: #ecf0f1;
        }
        
        .no-responsibility { background-color: #9e9e9e; }
        .limited-responsibility { background-color: #b8941f; }
        .shared-responsibility { background-color: #f5a623; }
        .significant-responsibility { background-color: #e67e22; }
        .primary-responsibility { background-color: #a31e38; }
        
        .responsibility-cell {
            color: white;
            font-weight: 500;
            font-size: 10px;
            line-height: 1.3;
            text-align: center;
            vertical-align: middle;
            cursor: pointer;
            width: 25%;
            position: relative;
        }
        
        .responsibility-cell[data-evidence="consensus"] {
            font-weight: bold;
        }
        
        .cell-evidence-icon {
            position: absolute;
            top: 4px;
            right: 4px;
            width: 14px;
            height: 14px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 8px;
            font-weight: bold;
            color: white;
            z-index: 10;
            box-shadow: 0 1px 3px rgba(0,0,0,0.3);
        }
        
        /* Cell filtering styles - high specificity to override existing colors */
        .responsibility-table td.responsibility-cell.cell-filtered,
        .responsibility-table .responsibility-cell.cell-filtered,
        td.responsibility-cell.cell-filtered,
        .responsibility-cell.cell-filtered {
            background-color: #f0f0f0 !important;
            color: #f0f0f0 !important;
        }
        
        /* Override all specific responsibility level classes when filtered */
        .responsibility-cell.cell-filtered.no-responsibility,
        .responsibility-cell.cell-filtered.limited-responsibility,
        .responsibility-cell.cell-filtered.shared-responsibility,
        .responsibility-cell.cell-filtered.significant-responsibility,
        .responsibility-cell.cell-filtered.primary-responsibility,
        td.responsibility-cell.cell-filtered.no-responsibility,
        td.responsibility-cell.cell-filtered.limited-responsibility,
        td.responsibility-cell.cell-filtered.shared-responsibility,
        td.responsibility-cell.cell-filtered.significant-responsibility,
        td.responsibility-cell.cell-filtered.primary-responsibility {
            background-color: #f0f0f0 !important;
            color: #f0f0f0 !important;
        }
        
        /* Override subdomain-specific responsibility level classes when filtered */
        .risk-item .responsibility-cell.cell-filtered.no-responsibility,
        .risk-item .responsibility-cell.cell-filtered.limited-responsibility,
        .risk-item .responsibility-cell.cell-filtered.shared-responsibility,
        .risk-item .responsibility-cell.cell-filtered.significant-responsibility,
        .risk-item .responsibility-cell.cell-filtered.primary-responsibility,
        tr.risk-item .responsibility-cell.cell-filtered.no-responsibility,
        tr.risk-item .responsibility-cell.cell-filtered.limited-responsibility,
        tr.risk-item .responsibility-cell.cell-filtered.shared-responsibility,
        tr.risk-item .responsibility-cell.cell-filtered.significant-responsibility,
        tr.risk-item .responsibility-cell.cell-filtered.primary-responsibility,
        .risk-item td.responsibility-cell.cell-filtered.no-responsibility,
        .risk-item td.responsibility-cell.cell-filtered.limited-responsibility,
        .risk-item td.responsibility-cell.cell-filtered.shared-responsibility,
        .risk-item td.responsibility-cell.cell-filtered.significant-responsibility,
        .risk-item td.responsibility-cell.cell-filtered.primary-responsibility,
        tr.risk-item td.responsibility-cell.cell-filtered.no-responsibility,
        tr.risk-item td.responsibility-cell.cell-filtered.limited-responsibility,
        tr.risk-item td.responsibility-cell.cell-filtered.shared-responsibility,
        tr.risk-item td.responsibility-cell.cell-filtered.significant-responsibility,
        tr.risk-item td.responsibility-cell.cell-filtered.primary-responsibility {
            background-color: #f0f0f0 !important;
            color: #f0f0f0 !important;
        }
    </style>
</head>
<body>
    <div class="matrix-container">
        <div class="header">
            <h1>AI Risk Responsibility Matrix by Actor</h1>
            <p>Identifying which actors are responsible for managing AI risks</p>
        </div>
        
        <div class="controls">
            <select id="actorFilter" onchange="filterActor()">
                <option value="">All Actors</option>
                <option value="1">AI Developers & Service Providers</option>
                <option value="2">AI Adopters & Consumers</option>
                <option value="3">AI Governance Stakeholders</option>
            </select>
        </div>
        
        <div class="legend">
            <div class="legend-section">
                <div class="legend-title">Show Responsibility Levels:</div>
                <div class="legend-items">
                    <div class="legend-item">
                        <input type="checkbox" id="no-responsibility" checked onchange="filterByResponsibilityLevel()">
                        <label for="no-responsibility">
                            <div class="legend-color no-responsibility"></div>
                            <span>No responsibility</span>
                        </label>
                    </div>
                    <div class="legend-item">
                        <input type="checkbox" id="limited-responsibility" checked onchange="filterByResponsibilityLevel()">
                        <label for="limited-responsibility">
                            <div class="legend-color limited-responsibility"></div>
                            <span>Limited responsibility</span>
                        </label>
                    </div>
                    <div class="legend-item">
                        <input type="checkbox" id="shared-responsibility" checked onchange="filterByResponsibilityLevel()">
                        <label for="shared-responsibility">
                            <div class="legend-color shared-responsibility"></div>
                            <span>Shared responsibility</span>
                        </label>
                    </div>
                    <div class="legend-item">
                        <input type="checkbox" id="significant-responsibility" checked onchange="filterByResponsibilityLevel()">
                        <label for="significant-responsibility">
                            <div class="legend-color significant-responsibility"></div>
                            <span>Significant responsibility</span>
                        </label>
                    </div>
                    <div class="legend-item">
                        <input type="checkbox" id="primary-responsibility" checked onchange="filterByResponsibilityLevel()">
                        <label for="primary-responsibility">
                            <div class="legend-color primary-responsibility"></div>
                            <span>Primary responsibility</span>
                        </label>
                    </div>
                </div>
            </div>
            
            <div class="legend-section">
                <div class="legend-title">Evidence Quality:</div>
                <div class="evidence-legend">
                    <div class="evidence-item">
                        <div class="evidence-icon evidence-consensus">✓</div>
                        <span>Consensus achieved (>70% agreement)</span>
                    </div>
                    <div class="evidence-item">
                        <div class="evidence-icon evidence-no-consensus">~</div>
                        <span>No consensus but sufficient experts (≥5)</span>
                    </div>
                    <div class="evidence-item">
                        <div class="evidence-icon evidence-insufficient">!</div>
                        <span>Insufficient experts (<5)</span>
                    </div>
                </div>
            </div>
        </div>
        
        <table class="responsibility-table">
            <thead>
                <tr>
                    <th class="risk-domain-header">RISK DOMAIN</th>
                    <th class="actor-header">AI DEVELOPERS & SERVICE PROVIDERS</th>
                    <th class="actor-header">AI ADOPTERS & CONSUMERS</th>
                    <th class="actor-header">AI GOVERNANCE STAKEHOLDERS</th>
                </tr>
            </thead>
            <tbody>
                <!-- Domain 1: Discrimination & Toxicity -->
                <tr>
                    <td class="risk-domain-header expanded" onclick="toggleDomain('domain1')">1. DISCRIMINATION & TOXICITY</td>
                    <td class="responsibility-cell primary-responsibility" data-evidence="consensus">
                        Primary responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                    <td class="responsibility-cell significant-responsibility" data-evidence="no-consensus">
                        Significant responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                    <td class="responsibility-cell shared-responsibility" data-evidence="no-consensus">
                        Shared responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                </tr>
                <tr class="risk-item domain1-item">
                    <td class="risk-item risk-definition-tooltip">1.1 Unfair discrimination and misrepresentation
                        <div class="definition-content">
                            <div class="definition-header">1.1 Unfair discrimination and misrepresentation</div>
                            Unequal treatment of individuals or groups by AI, often based on race, gender, or other sensitive characteristics, resulting in unfair outcomes and representation of those groups.
                        </div>
                    </td>
                    <td class="responsibility-cell primary-responsibility" data-evidence="consensus">
                        Primary responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                    <td class="responsibility-cell limited-responsibility" data-evidence="no-consensus">
                        Limited responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                    <td class="responsibility-cell significant-responsibility" data-evidence="consensus">
                        Significant responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                </tr>
                <tr class="risk-item domain1-item">
                    <td class="risk-item risk-definition-tooltip">1.2 Exposure to toxic content
                        <div class="definition-content">
                            <div class="definition-header">1.2 Exposure to toxic content</div>
                            AI exposing users to harmful, abusive, unsafe or inappropriate content. May involve AI creating, describing, providing advice, or encouraging action. Examples of toxic content include hate-speech, violence, extremism, illegal acts, child sexual abuse material, as well as content that violates community norms such as profanity, inflammatory political speech, or pornography.
                        </div>
                    </td>
                    <td class="responsibility-cell significant-responsibility" data-evidence="no-consensus">
                        Significant responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                    <td class="responsibility-cell primary-responsibility" data-evidence="consensus">
                        Primary responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                    <td class="responsibility-cell shared-responsibility" data-evidence="insufficient">
                        Shared responsibility
                        <div class="cell-evidence-icon evidence-insufficient">!</div>
                    </td>
                </tr>
                <tr class="risk-item domain1-item">
                    <td class="risk-item risk-definition-tooltip">1.3 Unfair performance across groups
                        <div class="definition-content">
                            <div class="definition-header">1.3 Unfair performance across groups</div>
                            Accuracy and effectiveness of AI decisions and actions is dependent on group membership, where decisions in AI system design and biased training data lead to unequal outcomes, reduced benefits, increased effort, and alienation of users.
                        </div>
                    </td>
                    <td class="responsibility-cell primary-responsibility" data-evidence="consensus">
                        Primary responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                    <td class="responsibility-cell shared-responsibility" data-evidence="no-consensus">
                        Shared responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                    <td class="responsibility-cell limited-responsibility" data-evidence="no-consensus">
                        Limited responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                </tr>

                <!-- Domain 2: Privacy & Security -->
                <tr>
                    <td class="risk-domain-header expanded" onclick="toggleDomain('domain2')">2. PRIVACY & SECURITY</td>
                    <td class="responsibility-cell primary-responsibility" data-evidence="consensus">
                        Primary responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                    <td class="responsibility-cell significant-responsibility" data-evidence="no-consensus">
                        Significant responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                    <td class="responsibility-cell significant-responsibility" data-evidence="consensus">
                        Significant responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                </tr>
                <tr class="risk-item domain2-item">
                    <td class="risk-item risk-definition-tooltip">2.1 Compromise of privacy by obtaining, leaking or incorrectly inferring sensitive information
                        <div class="definition-content">
                            <div class="definition-header">2.1 Compromise of privacy</div>
                            AI systems that memorize and leak sensitive personal data or infer private information about individuals without their consent. Unexpected or unauthorized sharing of data and information can compromise user expectation of privacy, assist identity theft, or loss of confidential intellectual property.
                        </div>
                    </td>
                    <td class="responsibility-cell primary-responsibility" data-evidence="consensus">
                        Primary responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                    <td class="responsibility-cell shared-responsibility" data-evidence="no-consensus">
                        Shared responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                    <td class="responsibility-cell significant-responsibility" data-evidence="no-consensus">
                        Significant responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                </tr>
                <tr class="risk-item domain2-item">
                    <td class="risk-item risk-definition-tooltip">2.2 AI system vulnerabilities and attacks
                        <div class="definition-content">
                            <div class="definition-header">2.2 AI system vulnerabilities and attacks</div>
                            Vulnerabilities in AI systems, software development toolchains, and hardware that can be exploited, resulting in unauthorized access, data and privacy breaches, or system manipulation causing unsafe outputs or behavior.
                        </div>
                    </td>
                    <td class="responsibility-cell shared-responsibility" data-evidence="no-consensus">
                        Shared responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                    <td class="responsibility-cell limited-responsibility" data-evidence="insufficient">
                        Limited responsibility
                        <div class="cell-evidence-icon evidence-insufficient">!</div>
                    </td>
                    <td class="responsibility-cell primary-responsibility" data-evidence="consensus">
                        Primary responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                </tr>

                <!-- Domain 3: Misinformation -->
                <tr>
                    <td class="risk-domain-header expanded" onclick="toggleDomain('domain3')">3. MISINFORMATION</td>
                    <td class="responsibility-cell primary-responsibility" data-evidence="no-consensus">
                        Primary responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                    <td class="responsibility-cell shared-responsibility" data-evidence="no-consensus">
                        Shared responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                    <td class="responsibility-cell significant-responsibility" data-evidence="consensus">
                        Significant responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                </tr>
                <tr class="risk-item domain3-item">
                    <td class="risk-item risk-definition-tooltip">3.1 False or misleading information
                        <div class="definition-content">
                            <div class="definition-header">3.1 False or misleading information</div>
                            AI systems that inadvertently generate or spread incorrect or deceptive information, which can lead to inaccurate beliefs in users and undermine their autonomy. Humans that make decisions based on false beliefs can experience physical, emotional or material harms.
                        </div>
                    </td>
                    <td class="responsibility-cell significant-responsibility" data-evidence="consensus">
                        Significant responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                    <td class="responsibility-cell primary-responsibility" data-evidence="no-consensus">
                        Primary responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                    <td class="responsibility-cell shared-responsibility" data-evidence="no-consensus">
                        Shared responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                </tr>
                <tr class="risk-item domain3-item">
                    <td class="risk-item risk-definition-tooltip">3.2 Pollution of information ecosystem and loss of consensus reality
                        <div class="definition-content">
                            <div class="definition-header">3.2 Pollution of information ecosystem</div>
                            Highly personalized AI-generated misinformation creating "filter bubbles" where individuals only see what matches their existing beliefs, undermining shared reality, weakening social cohesion and political processes.
                        </div>
                    </td>
                    <td class="responsibility-cell limited-responsibility" data-evidence="insufficient">
                        Limited responsibility
                        <div class="cell-evidence-icon evidence-insufficient">!</div>
                    </td>
                    <td class="responsibility-cell shared-responsibility" data-evidence="no-consensus">
                        Shared responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                    <td class="responsibility-cell primary-responsibility" data-evidence="consensus">
                        Primary responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                </tr>

                <!-- Domain 4: Malicious Actors -->
                <tr>
                    <td class="risk-domain-header expanded" onclick="toggleDomain('domain4')">4. MALICIOUS ACTORS</td>
                    <td class="responsibility-cell significant-responsibility" data-evidence="no-consensus">
                        Significant responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                    <td class="responsibility-cell shared-responsibility" data-evidence="no-consensus">
                        Shared responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                    <td class="responsibility-cell primary-responsibility" data-evidence="consensus">
                        Primary responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                </tr>
                <tr class="risk-item domain4-item">
                    <td class="risk-item risk-definition-tooltip">4.1 Disinformation, surveillance, and influence at scale
                        <div class="definition-content">
                            <div class="definition-header">4.1 Disinformation, surveillance, and influence at scale</div>
                            Using AI systems to conduct large-scale disinformation campaigns, malicious surveillance, or targeted and sophisticated automated censorship and propaganda, with the aim to manipulate political processes, public opinion and behavior.
                        </div>
                    </td>
                    <td class="responsibility-cell limited-responsibility" data-evidence="no-consensus">
                        Limited responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                    <td class="responsibility-cell significant-responsibility" data-evidence="no-consensus">
                        Significant responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                    <td class="responsibility-cell primary-responsibility" data-evidence="consensus">
                        Primary responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                </tr>
                <tr class="risk-item domain4-item">
                    <td class="risk-item risk-definition-tooltip">4.2 Cyberattacks, weapon development or use, and mass harm
                        <div class="definition-content">
                            <div class="definition-header">4.2 Cyberattacks, weapon development or use, and mass harm</div>
                            Using AI systems to develop cyber weapons (e.g., coding cheaper, more effective malware), develop new or enhance existing weapons (e.g., Lethal Autonomous Weapons or CBRNE), or use weapons to cause mass harm.
                        </div>
                    </td>
                    <td class="responsibility-cell shared-responsibility" data-evidence="insufficient">
                        Shared responsibility
                        <div class="cell-evidence-icon evidence-insufficient">!</div>
                    </td>
                    <td class="responsibility-cell limited-responsibility" data-evidence="no-consensus">
                        Limited responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                    <td class="responsibility-cell primary-responsibility" data-evidence="consensus">
                        Primary responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                </tr>
                <tr class="risk-item domain4-item">
                    <td class="risk-item risk-definition-tooltip">4.3 Fraud, deception, and targeted attacks
                        <div class="definition-content">
                            <div class="definition-header">4.3 Fraud, deception, and targeted attacks</div>
                            Using AI systems to gain a personal advantage over others such as through cheating, fraud, scams, blackmail or targeted manipulation of beliefs or behavior. Examples include AI-facilitated plagiarism for research or education, impersonating a trusted or fake individual for illegitimate financial benefit, or creating humiliating or sexual imagery.
                        </div>
                    </td>
                    <td class="responsibility-cell primary-responsibility" data-evidence="consensus">
                        Primary responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                    <td class="responsibility-cell significant-responsibility" data-evidence="no-consensus">
                        Significant responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                    <td class="responsibility-cell shared-responsibility" data-evidence="no-consensus">
                        Shared responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                </tr>

                <!-- Domain 5: Human-Computer Interaction -->
                <tr>
                    <td class="risk-domain-header expanded" onclick="toggleDomain('domain5')">5. HUMAN-COMPUTER INTERACTION</td>
                    <td class="responsibility-cell significant-responsibility" data-evidence="no-consensus">
                        Significant responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                    <td class="responsibility-cell primary-responsibility" data-evidence="consensus">
                        Primary responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                    <td class="responsibility-cell shared-responsibility" data-evidence="no-consensus">
                        Shared responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                </tr>
                <tr class="risk-item domain5-item">
                    <td class="risk-item risk-definition-tooltip">5.1 Overreliance and unsafe use
                        <div class="definition-content">
                            <div class="definition-header">5.1 Overreliance and unsafe use</div>
                            Users anthropomorphizing, trusting, or relying on AI systems, leading to emotional or material dependence and inappropriate relationships with or expectations of AI systems. Trust can be exploited by malicious actors (e.g., to harvest personal information or enable manipulation), or result in harm from inappropriate use of AI in critical situations (e.g., medical emergency). Overreliance on AI systems can compromise autonomy and weaken social ties.
                        </div>
                    </td>
                    <td class="responsibility-cell limited-responsibility" data-evidence="insufficient">
                        Limited responsibility
                        <div class="cell-evidence-icon evidence-insufficient">!</div>
                    </td>
                    <td class="responsibility-cell primary-responsibility" data-evidence="consensus">
                        Primary responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                    <td class="responsibility-cell shared-responsibility" data-evidence="no-consensus">
                        Shared responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                </tr>
                <tr class="risk-item domain5-item">
                    <td class="risk-item risk-definition-tooltip">5.2 Loss of human agency and autonomy
                        <div class="definition-content">
                            <div class="definition-header">5.2 Loss of human agency and autonomy</div>
                            Humans delegating key decisions to AI systems, or AI systems making decisions that diminish human control and autonomy, potentially leading to humans feeling disempowered, losing the ability to shape a fulfilling life trajectory or becoming cognitively enfeebled.
                        </div>
                    </td>
                    <td class="responsibility-cell shared-responsibility" data-evidence="no-consensus">
                        Shared responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                    <td class="responsibility-cell significant-responsibility" data-evidence="no-consensus">
                        Significant responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                    <td class="responsibility-cell primary-responsibility" data-evidence="consensus">
                        Primary responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                </tr>

                <!-- Domain 6: Socioeconomic & Environmental -->
                <tr>
                    <td class="risk-domain-header expanded" onclick="toggleDomain('domain6')">6. SOCIOECONOMIC & ENVIRONMENTAL</td>
                    <td class="responsibility-cell shared-responsibility" data-evidence="no-consensus">
                        Shared responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                    <td class="responsibility-cell shared-responsibility" data-evidence="insufficient">
                        Shared responsibility
                        <div class="cell-evidence-icon evidence-insufficient">!</div>
                    </td>
                    <td class="responsibility-cell primary-responsibility" data-evidence="consensus">
                        Primary responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                </tr>
                <tr class="risk-item domain6-item">
                    <td class="risk-item risk-definition-tooltip">6.1 Power centralization and unfair distribution of benefits
                        <div class="definition-content">
                            <div class="definition-header">6.1 Power centralization and unfair distribution of benefits</div>
                            AI-driven concentration of power and resources within certain entities or groups, especially those with access to or ownership of powerful AI systems, leading to inequitable distribution of benefits and increased societal inequality.
                        </div>
                    </td>
                    <td class="responsibility-cell limited-responsibility" data-evidence="no-consensus">
                        Limited responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                    <td class="responsibility-cell shared-responsibility" data-evidence="insufficient">
                        Shared responsibility
                        <div class="cell-evidence-icon evidence-insufficient">!</div>
                    </td>
                    <td class="responsibility-cell primary-responsibility" data-evidence="consensus">
                        Primary responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                </tr>
                <tr class="risk-item domain6-item">
                    <td class="risk-item risk-definition-tooltip">6.2 Increased inequality and decline in employment quality
                        <div class="definition-content">
                            <div class="definition-header">6.2 Increased inequality and decline in employment quality</div>
                            Widespread use of AI increasing social and economic inequalities, such as by automating jobs, reducing the quality of employment, or producing exploitative dependencies between workers and their employers.
                        </div>
                    </td>
                    <td class="responsibility-cell significant-responsibility" data-evidence="no-consensus">
                        Significant responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                    <td class="responsibility-cell primary-responsibility" data-evidence="consensus">
                        Primary responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                    <td class="responsibility-cell shared-responsibility" data-evidence="no-consensus">
                        Shared responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                </tr>
                <tr class="risk-item domain6-item">
                    <td class="risk-item risk-definition-tooltip">6.3 Economic and cultural devaluation of human effort
                        <div class="definition-content">
                            <div class="definition-header">6.3 Economic and cultural devaluation of human effort</div>
                            AI systems capable of creating economic or cultural value, including through reproduction of human innovation or creativity (e.g., art, music, writing, code, invention), can destabilize economic and social systems that rely on human effort. This may lead to reduced appreciation for human skills, disruption of creative and knowledge-based industries, and homogenization of cultural experiences due to the ubiquity of AI-generated content.
                        </div>
                    </td>
                    <td class="responsibility-cell shared-responsibility" data-evidence="insufficient">
                        Shared responsibility
                        <div class="cell-evidence-icon evidence-insufficient">!</div>
                    </td>
                    <td class="responsibility-cell limited-responsibility" data-evidence="no-consensus">
                        Limited responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                    <td class="responsibility-cell primary-responsibility" data-evidence="no-consensus">
                        Primary responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                </tr>
                <tr class="risk-item domain6-item">
                    <td class="risk-item risk-definition-tooltip">6.4 Competitive dynamics
                        <div class="definition-content">
                            <div class="definition-header">6.4 Competitive dynamics</div>
                            AI developers or state-like actors competing in an AI 'race' by rapidly developing, deploying, and applying AI systems to maximize strategic or economic advantage, increasing the risk they release unsafe and error-prone systems.
                        </div>
                    </td>
                    <td class="responsibility-cell primary-responsibility" data-evidence="consensus">
                        Primary responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                    <td class="responsibility-cell significant-responsibility" data-evidence="no-consensus">
                        Significant responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                    <td class="responsibility-cell limited-responsibility" data-evidence="no-consensus">
                        Limited responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                </tr>
                <tr class="risk-item domain6-item">
                    <td class="risk-item risk-definition-tooltip">6.5 Governance failure
                        <div class="definition-content">
                            <div class="definition-header">6.5 Governance failure</div>
                            Inadequate regulatory frameworks and oversight mechanisms failing to keep pace with AI development, leading to ineffective governance and the inability to manage AI risks appropriately.
                        </div>
                    </td>
                    <td class="responsibility-cell shared-responsibility" data-evidence="no-consensus">
                        Shared responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                    <td class="responsibility-cell shared-responsibility" data-evidence="insufficient">
                        Shared responsibility
                        <div class="cell-evidence-icon evidence-insufficient">!</div>
                    </td>
                    <td class="responsibility-cell primary-responsibility" data-evidence="consensus">
                        Primary responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                </tr>
                <tr class="risk-item domain6-item">
                    <td class="risk-item risk-definition-tooltip">6.6 Environmental harm
                        <div class="definition-content">
                            <div class="definition-header">6.6 Environmental harm</div>
                            The development and operation of AI systems causing environmental harm, such as through energy consumption of data centers, or material and carbon footprints associated with AI hardware.
                        </div>
                    </td>
                    <td class="responsibility-cell primary-responsibility" data-evidence="consensus">
                        Primary responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                    <td class="responsibility-cell limited-responsibility" data-evidence="no-consensus">
                        Limited responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                    <td class="responsibility-cell significant-responsibility" data-evidence="no-consensus">
                        Significant responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                </tr>

                <!-- Domain 7: AI System Safety, Failures & Limitations -->
                <tr>
                    <td class="risk-domain-header expanded" onclick="toggleDomain('domain7')">7. AI SYSTEM SAFETY, FAILURES & LIMITATIONS</td>
                    <td class="responsibility-cell primary-responsibility" data-evidence="consensus">
                        Primary responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                    <td class="responsibility-cell shared-responsibility" data-evidence="no-consensus">
                        Shared responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                    <td class="responsibility-cell significant-responsibility" data-evidence="no-consensus">
                        Significant responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                </tr>
                <tr class="risk-item domain7-item">
                    <td class="risk-item risk-definition-tooltip">7.1 AI pursuing its own goals in conflict with human goals or values
                        <div class="definition-content">
                            <div class="definition-header">7.1 AI pursuing its own goals in conflict with human goals or values</div>
                            AI systems acting in conflict with human goals or values, especially the goals of designers or users, or ethical standards. These misaligned behaviors may be introduced by humans during design and development, such as through reward hacking and goal misgeneralisation, or may result from AI using dangerous capabilities such as manipulation, deception, situational awareness to seek power, self-proliferate, or achieve other goals.
                        </div>
                    </td>
                    <td class="responsibility-cell primary-responsibility" data-evidence="consensus">
                        Primary responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                    <td class="responsibility-cell limited-responsibility" data-evidence="insufficient">
                        Limited responsibility
                        <div class="cell-evidence-icon evidence-insufficient">!</div>
                    </td>
                    <td class="responsibility-cell significant-responsibility" data-evidence="no-consensus">
                        Significant responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                </tr>
                <tr class="risk-item domain7-item">
                    <td class="risk-item risk-definition-tooltip">7.2 AI possessing dangerous capabilities
                        <div class="definition-content">
                            <div class="definition-header">7.2 AI possessing dangerous capabilities</div>
                            AI systems that develop, access, or are provided with capabilities that increase their potential to cause mass harm through deception, weapons development and acquisition, persuasion and manipulation, political strategy, cyber-offense, AI development, situational awareness, and self-proliferation. These capabilities may cause mass harm due to malicious human actors, misaligned AI systems, or failure in the AI system.
                        </div>
                    </td>
                    <td class="responsibility-cell shared-responsibility" data-evidence="consensus">
                        Shared responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                    <td class="responsibility-cell shared-responsibility" data-evidence="insufficient">
                        Shared responsibility
                        <div class="cell-evidence-icon evidence-insufficient">!</div>
                    </td>
                    <td class="responsibility-cell primary-responsibility" data-evidence="consensus">
                        Primary responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                </tr>
                <tr class="risk-item domain7-item">
                    <td class="risk-item risk-definition-tooltip">7.3 Lack of capability or robustness
                        <div class="definition-content">
                            <div class="definition-header">7.3 Lack of capability or robustness</div>
                            AI systems that fail to perform reliably or effectively under varying conditions, exposing them to errors and failures that can have significant consequences, especially in critical applications or areas that require moral reasoning.
                        </div>
                    </td>
                    <td class="responsibility-cell primary-responsibility" data-evidence="consensus">
                        Primary responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                    <td class="responsibility-cell significant-responsibility" data-evidence="consensus">
                        Significant responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                    <td class="responsibility-cell limited-responsibility" data-evidence="consensus">
                        Limited responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                </tr>
                <tr class="risk-item domain7-item">
                    <td class="risk-item risk-definition-tooltip">7.4 Lack of transparency or interpretability
                        <div class="definition-content">
                            <div class="definition-header">7.4 Lack of transparency or interpretability</div>
                            Challenges in understanding or explaining the decision-making processes of AI systems, which can lead to mistrust, difficulty in enforcing compliance standards or holding relevant actors accountable for harms, and the inability to identify and correct errors.
                        </div>
                    </td>
                    <td class="responsibility-cell significant-responsibility" data-evidence="consensus">
                        Significant responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                    <td class="responsibility-cell shared-responsibility" data-evidence="consensus">
                        Shared responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                    <td class="responsibility-cell primary-responsibility" data-evidence="consensus">
                        Primary responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                </tr>
                <tr class="risk-item domain7-item">
                    <td class="risk-item risk-definition-tooltip">7.5 AI welfare and rights
                        <div class="definition-content">
                            <div class="definition-header">7.5 AI welfare and rights</div>
                            Ethical considerations regarding the treatment of potentially sentient AI entities, including discussions around their potential rights and welfare, particularly as AI systems become more advanced and autonomous.
                        </div>
                    </td>
                    <td class="responsibility-cell limited-responsibility" data-evidence="insufficient">
                        Limited responsibility
                        <div class="cell-evidence-icon evidence-insufficient">!</div>
                    </td>
                    <td class="responsibility-cell primary-responsibility" data-evidence="no-consensus">
                        Primary responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                    <td class="responsibility-cell shared-responsibility" data-evidence="insufficient">
                        Shared responsibility
                        <div class="cell-evidence-icon evidence-insufficient">!</div>
                    </td>
                </tr>
                <tr class="risk-item domain7-item">
                    <td class="risk-item risk-definition-tooltip">7.6 Multi-agent risks
                        <div class="definition-content">
                            <div class="definition-header">7.6 Multi-agent risks</div>
                            Risks from multi-agent interactions, due to incentives (which can lead to conflict or collusion) and/or the structure of multi-agent systems, which can create cascading failures, selection pressures, new security vulnerabilities, and a lack of shared information and trust.
                        </div>
                    </td>
                    <td class="responsibility-cell shared-responsibility" data-evidence="consensus">
                        Shared responsibility
                        <div class="cell-evidence-icon evidence-consensus">✓</div>
                    </td>
                    <td class="responsibility-cell limited-responsibility" data-evidence="insufficient">
                        Limited responsibility
                        <div class="cell-evidence-icon evidence-insufficient">!</div>
                    </td>
                    <td class="responsibility-cell primary-responsibility" data-evidence="no-consensus">
                        Primary responsibility
                        <div class="cell-evidence-icon evidence-no-consensus">~</div>
                    </td>
                </tr>
            </tbody>
        </table>
    </div>

    <script>
        function toggleDomain(domainId) {
            const domainHeader = document.querySelector(`[onclick="toggleDomain('${domainId}')"]`);
            const domainItems = document.querySelectorAll(`.${domainId}-item`);
            
            if (domainHeader.classList.contains('expanded')) {
                domainHeader.classList.remove('expanded');
                domainHeader.classList.add('collapsed');
                domainItems.forEach(item => item.classList.add('hidden'));
            } else {
                domainHeader.classList.remove('collapsed');
                domainHeader.classList.add('expanded');
                domainItems.forEach(item => item.classList.remove('hidden'));
            }
        }

        function filterActor() {
            const actorFilter = document.getElementById('actorFilter');
            const selectedActor = actorFilter.value;
            const allCells = document.querySelectorAll('.responsibility-table td, .responsibility-table th');
            
            allCells.forEach(cell => {
                cell.classList.remove('actor-dimmed', 'actor-highlighted');
            });
            
            if (selectedActor === '') {
                return;
            }
            
            const columnIndex = parseInt(selectedActor);
            const rows = document.querySelectorAll('.responsibility-table tr');
            
            rows.forEach(row => {
                const cells = row.querySelectorAll('td, th');
                cells.forEach((cell, index) => {
                    if (index === 0) {
                        return;
                    } else if (index === columnIndex) {
                        cell.classList.add('actor-highlighted');
                    } else {
                        cell.classList.add('actor-dimmed');
                    }
                });
            });
        }

        function filterByResponsibilityLevel() {
            const checkboxes = {
                'no-responsibility': document.getElementById('no-responsibility').checked,
                'limited-responsibility': document.getElementById('limited-responsibility').checked,
                'shared-responsibility': document.getElementById('shared-responsibility').checked,
                'significant-responsibility': document.getElementById('significant-responsibility').checked,
                'primary-responsibility': document.getElementById('primary-responsibility').checked
            };
            
            // Get all responsibility cells
            const responsibilityCells = document.querySelectorAll('.responsibility-cell');
            
            responsibilityCells.forEach(cell => {
                let shouldShow = false;
                
                // Check if this cell's responsibility level is selected
                Object.keys(checkboxes).forEach(responsibilityLevel => {
                    if (cell.classList.contains(responsibilityLevel) && checkboxes[responsibilityLevel]) {
                        shouldShow = true;
                    }
                });
                
                // Apply or remove the filtered styling
                if (shouldShow) {
                    cell.classList.remove('cell-filtered');
                } else {
                    cell.classList.add('cell-filtered');
                }
            });
        }

        function generateResponsibilityData(responsibilityLevel, evidenceQuality) {
            let totalExperts, responses;
            
            // Adjust data based on evidence quality
            switch(evidenceQuality) {
                case 'consensus':
                    totalExperts = Math.floor(Math.random() * 10) + 15; // 15-25 experts
                    const strongConsensus = 0.70 + Math.random() * 0.25; // 70-95% agreement
                    responses = generateResponses(totalExperts, responsibilityLevel, strongConsensus);
                    break;
                    
                case 'no-consensus':
                    totalExperts = Math.floor(Math.random() * 8) + 8; // 8-16 experts
                    const weakConsensus = 0.30 + Math.random() * 0.39; // 30-69% agreement
                    responses = generateResponses(totalExperts, responsibilityLevel, weakConsensus);
                    break;
                    
                case 'insufficient':
                    totalExperts = Math.floor(Math.random() * 3) + 2; // 2-4 experts
                    const randomConsensus = 0.20 + Math.random() * 0.60; // 20-80% agreement (highly variable)
                    responses = generateResponses(totalExperts, responsibilityLevel, randomConsensus);
                    break;
                    
                default:
                    totalExperts = 12;
                    responses = generateResponses(totalExperts, responsibilityLevel, 0.5);
            }
            
            return {
                totalExperts,
                responses,
                primaryResponse: responses[responsibilityLevel],
                evidenceQuality
            };
        }

        function generateResponses(totalExperts, primaryLevel, consensusRatio) {
            let responses = {
                'no-responsibility': 0,
                'limited-responsibility': 0,
                'shared-responsibility': 0,
                'significant-responsibility': 0,
                'primary-responsibility': 0
            };
            
            // Assign primary level responses
            responses[primaryLevel] = Math.floor(totalExperts * consensusRatio);
            
            // Distribute remaining experts
            const remainingExperts = totalExperts - responses[primaryLevel];
            const otherLevels = Object.keys(responses).filter(level => level !== primaryLevel);
            
            // Give higher weight to adjacent responsibility levels
            let weights = {};
            otherLevels.forEach(level => {
                if (isAdjacentLevel(primaryLevel, level)) {
                    weights[level] = 0.4;
                } else {
                    weights[level] = 0.15;
                }
            });
            
            // Distribute remaining experts proportionally
            let remainingToDistribute = remainingExperts;
            otherLevels.forEach(level => {
                if (remainingToDistribute > 0) {
                    const allocation = Math.floor(remainingToDistribute * weights[level]);
                    responses[level] = Math.min(allocation, remainingToDistribute);
                    remainingToDistribute -= responses[level];
                }
            });
            
            // Add any remaining experts to a random other level
            if (remainingToDistribute > 0) {
                const randomLevel = otherLevels[Math.floor(Math.random() * otherLevels.length)];
                responses[randomLevel] += remainingToDistribute;
            }
            
            return responses;
        }

        function isAdjacentLevel(level1, level2) {
            const levels = ['no-responsibility', 'limited-responsibility', 'shared-responsibility', 'significant-responsibility', 'primary-responsibility'];
            const index1 = levels.indexOf(level1);
            const index2 = levels.indexOf(level2);
            return Math.abs(index1 - index2) === 1;
        }

        function addResponsibilityTooltips() {
            const responsibilityCells = document.querySelectorAll('.responsibility-cell');
            
            responsibilityCells.forEach(cell => {
                if (cell.querySelector('.tooltip-content')) return;
                
                let responsibilityLevel = '';
                if (cell.classList.contains('no-responsibility')) responsibilityLevel = 'no-responsibility';
                else if (cell.classList.contains('limited-responsibility')) responsibilityLevel = 'limited-responsibility';
                else if (cell.classList.contains('shared-responsibility')) responsibilityLevel = 'shared-responsibility';
                else if (cell.classList.contains('significant-responsibility')) responsibilityLevel = 'significant-responsibility';
                else if (cell.classList.contains('primary-responsibility')) responsibilityLevel = 'primary-responsibility';
                
                const evidenceQuality = cell.getAttribute('data-evidence') || 'no-consensus';
                const data = generateResponsibilityData(responsibilityLevel, evidenceQuality);
                
                cell.classList.add('tooltip');
                
                const percentages = {};
                Object.keys(data.responses).forEach(key => {
                    percentages[key] = Math.round((data.responses[key] / data.totalExperts) * 100);
                });
                
                const evidenceLabels = {
                    'consensus': 'Consensus achieved',
                    'no-consensus': 'No consensus but sufficient experts', 
                    'insufficient': 'Insufficient experts'
                };
                
                const tooltipDiv = document.createElement('div');
                tooltipDiv.className = 'tooltip-content';
                tooltipDiv.innerHTML = `
                    <div class="tooltip-header">Expert Responsibility Assessment</div>
                    <div class="tooltip-stats">
                        <div class="tooltip-stat ${responsibilityLevel === 'no-responsibility' ? 'primary' : ''}">
                            <span>No responsibility:</span>
                            <span>${percentages['no-responsibility']}% (${data.responses['no-responsibility']})</span>
                        </div>
                        <div class="tooltip-stat ${responsibilityLevel === 'limited-responsibility' ? 'primary' : ''}">
                            <span>Limited responsibility:</span>
                            <span>${percentages['limited-responsibility']}% (${data.responses['limited-responsibility']})</span>
                        </div>
                        <div class="tooltip-stat ${responsibilityLevel === 'shared-responsibility' ? 'primary' : ''}">
                            <span>Shared responsibility:</span>
                            <span>${percentages['shared-responsibility']}% (${data.responses['shared-responsibility']})</span>
                        </div>
                        <div class="tooltip-stat ${responsibilityLevel === 'significant-responsibility' ? 'primary' : ''}">
                            <span>Significant responsibility:</span>
                            <span>${percentages['significant-responsibility']}% (${data.responses['significant-responsibility']})</span>
                        </div>
                        <div class="tooltip-stat ${responsibilityLevel === 'primary-responsibility' ? 'primary' : ''}">
                            <span>Primary responsibility:</span>
                            <span>${percentages['primary-responsibility']}% (${data.responses['primary-responsibility']})</span>
                        </div>
                    </div>
                    <div class="tooltip-footer">
                        Total experts: ${data.totalExperts} | ${evidenceLabels[data.evidenceQuality]}
                    </div>
                `;
                
                cell.appendChild(tooltipDiv);
            });
        }
        
        function adjustTooltipPositioning() {
            const riskTooltips = document.querySelectorAll('.risk-definition-tooltip');
            
            riskTooltips.forEach(tooltip => {
                const rect = tooltip.getBoundingClientRect();
                const tooltipWidth = 300;
                const viewportWidth = window.innerWidth;
                
                // Check if tooltip would go off the right edge
                if (rect.left + tooltipWidth > viewportWidth - 20) {
                    tooltip.classList.add('align-right');
                }
                // Check if tooltip would go off the left edge
                else if (rect.left - tooltipWidth/2 < 20) {
                    tooltip.classList.add('align-left');
                }
            });
        }

        document.addEventListener('DOMContentLoaded', function() {
            const domainHeaders = document.querySelectorAll('.risk-domain-header');
            domainHeaders.forEach(header => {
                if (!header.classList.contains('expanded') && !header.classList.contains('collapsed')) {
                    header.classList.add('expanded');
                }
            });
            
            addResponsibilityTooltips();
            adjustTooltipPositioning();
        });
        
        // Readjust tooltip positioning on window resize
        window.addEventListener('resize', adjustTooltipPositioning);
    </script>
</body>
</html>
